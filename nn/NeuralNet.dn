component provides nn.NeuralNet requires gpu.LogicalComputeDevice, io.Output out, data.DecUtil du, data.IntUtil iu, data.adt.HashTable, util.Math math {

    LogicalComputeDevice myDevice

    int inputSize
    int trainingSetSize
    int outputSize
    int hiddenLayerSizes[]

    int activationMethod
    int trainingAlgo
    int itterations
    dec learningRate

    dec trainingSet[][]
    int trainLabels[]

    HashTable labelsToOutputs
    bool tableCreated

    Network net
    Gradient grad

    NeuralNet:NeuralNet(store LogicalComputeDevice dev) {
        myDevice = dev
        tableCreated = false
        //load required programs
        myDevice.loadProgram("/home/ben/Documents/PhD/DanaOpenCL/resources-ext/opencl_kernels/randmat.cl", "randMat")
        myDevice.loadProgram("/home/ben/Documents/PhD/DanaOpenCL/resources-ext/opencl_kernels/randvect.cl", "randVect")
        myDevice.loadProgram("/home/ben/Documents/PhD/DanaOpenCL/resources-ext/opencl_kernels/weightedSum.cl", "weightedSum")
        myDevice.loadProgram("/home/ben/Documents/PhD/DanaOpenCL/resources-ext/opencl_kernels/relu.cl", "relu")
        myDevice.loadProgram("/home/ben/Documents/PhD/DanaOpenCL/resources-ext/opencl_kernels/biasedWeightedSumMat.cl", "dotMat")
    }

    /*
    void createTable() {
        labelsToOutputs = new HashTable()
        for (int i = 0; i < outputSize; i++) {
            Vector desiredOutput = new Vector()
            dec desiredOutputDec[] = new dec[outputSize]
            for (k = 0; k < outputSize; k++) {
                if (i == k) {desiredOutputDec[k] = 1.0}
                else {desiredOutputDec[k] = 0.0}
            }
            desiredOutput.fvalues = desiredOutputDec
            labelsToOutputs.put(iu.makeString(i), desiredOutput)
        }
    }
    */

    void NeuralNet:setInputVectorSize(int sz){
        inputSize = sz
    }
    void NeuralNet:setTrainingSetSize(int sz){
        trainingSetSize = sz
    }
    void NeuralNet:setOutputVectorSize(int sz){
        outputSize = sz
    }
    void NeuralNet:setHiddenLayerSizes(int sz[]){
        hiddenLayerSizes = sz
    }
    void NeuralNet:setHiddenLayerActivation(int layer, int function){
        activationMethod = function
    }
    void NeuralNet:setTrainingAlgo(int algo){
        trainingAlgo = algo
    }
    void NeuralNet:setTrainingLabels(int labels[]) {
        trainLabels = labels
    }
    void NeuralNet:setItterations(int it) {
        itterations = it
    }
    void NeuralNet:setLearningRate(dec lr) {
        learningRate = lr
    }

    void forwardProp() {
        for (int i = 0; i <= hiddenLayerSizes.arrayLength; i++) {
            if (i == 0) {
                String params[] = new String[](new String("b$(i)"), new String("Input"), new String("w$(i)"), new String("z$(i)"))
                myDevice.runProgram("dotMat", params)  
            }
            else {
                String params[] = new String[](new String("b$(i)"), new String("a$(i-1)"), new String("w$(i)"), new String("z$(i)"))
                myDevice.runProgram("dotMat", params)
            }
            String params[] = new String[](new String("z$(i)"), new String("a$(i)"))
            myDevice.runProgram("relu", params)
        }
    }

    void cost() {

    }

    void backProp() {
        for (int i = hiddenLayerSizes.arrayLength; i >= 0; i--) {
            
        }
    }

    void updateNetwork() {

    }

    void gradientDecent() {
        myDevice.createMatrix("Input", FLOAT, 2, new int[](trainingSet.arrayLength, trainingSet[0].arrayLength))
        for (int i = 0; i < itterations; i++) {
            forwardProp()
            cost()
            backProp()
            updateNetwork()
        }
    }

    void initNetork() {
        net = new Network()
        net.W = new String[hiddenLayerSizes.arrayLength+1]
        net.B = new String[hiddenLayerSizes.arrayLength+1]
        net.Z = new String[hiddenLayerSizes.arrayLength+1]
        net.A = new String[hiddenLayerSizes.arrayLength+1]

        grad = new Gradient()
        grad.dW = new String[hiddenLayerSizes.arrayLength+1]
        grad.dB = new String[hiddenLayerSizes.arrayLength+1]
        grad.dZ = new String[hiddenLayerSizes.arrayLength+1]

        for (int i = 0; i <= hiddenLayerSizes.arrayLength; i++) {
            if (i == 0) {
                myDevice.createMatrix("w$(i)", FLOAT, 2, new int[](trainingSetSize, hiddenLayerSizes[i]))
                myDevice.createArray("b$(i)", FLOAT, hiddenLayerSizes[i])
                myDevice.createMatrix("z$(i)", FLOAT, 2, new int[](trainingSetSize, hiddenLayerSizes[i]))
                myDevice.createMatrix("a$(i)", FLOAT, 2, new int[](trainingSetSize, hiddenLayerSizes[i]))

                myDevice.createMatrix("dw$(i)", FLOAT, 2, new int[](trainingSetSize, hiddenLayerSizes[i]))
                myDevice.createArray("db$(i)", FLOAT, hiddenLayerSizes[i])
                myDevice.createMatrix("dz$(i)", FLOAT, 2, new int[](trainingSetSize, hiddenLayerSizes[i]))
            }
            else if (i == hiddenLayerSizes.arrayLength) {
                myDevice.createMatrix("w$(i)", FLOAT, 2, new int[](hiddenLayerSizes[i-1], outputSize))
                myDevice.createArray("b$(i)", FLOAT, outputSize)
                myDevice.createMatrix("z$(i)", FLOAT, 2, new int[](trainingSetSize, outputSize))
                myDevice.createMatrix("a$(i)", FLOAT, 2, new int[](trainingSetSize, outputSize))

                myDevice.createMatrix("dw$(i)", FLOAT, 2, new int[](hiddenLayerSizes[i-1], outputSize))
                myDevice.createArray("db$(i)", FLOAT, outputSize)
                myDevice.createMatrix("dz$(i)", FLOAT, 2, new int[](trainingSetSize, outputSize))
            }
            else {
                myDevice.createMatrix("w$(i)", FLOAT, 2, new int[](hiddenLayerSizes[i-1], hiddenLayerSizes[i]))
                myDevice.createArray("b$(i)", FLOAT, hiddenLayerSizes[i])
                myDevice.createMatrix("z$(i)", FLOAT, 2, new int[](trainingSetSize, hiddenLayerSizes[i]))
                myDevice.createMatrix("a$(i)", FLOAT, 2, new int[](trainingSetSize, hiddenLayerSizes[i]))

                myDevice.createMatrix("dw$(i)", FLOAT, 2, new int[](hiddenLayerSizes[i-1], hiddenLayerSizes[i]))
                myDevice.createArray("db$(i)", FLOAT, hiddenLayerSizes[i])
                myDevice.createMatrix("dz$(i)", FLOAT, 2, new int[](hiddenLayerSizes[i-1], hiddenLayerSizes[i]))
            }

            net.W[i] = new String("w$(i)")
            net.B[i] = new String("b$(i)")
            net.Z[i] = new String("z$(i)")
            net.A[i] = new String("a$(i)")

            grad.dW[i] = new String("dw$(i)")
            grad.dB[i] = new String("db$(i)")
            grad.dZ[i] = new String("dz$(i)")

            String params[] = new String[](new String("const:12"), new String("w$(i)"))
            myDevice.runProgram("randMat", params)

            params = new String[](new String("const:12"), new String("b$(i)"))
            myDevice.runProgram("randVect", params)
        }
    }

    void NeuralNet:train(store dec trainSet[][]){
        trainingSet = trainSet
        initNetork()
        if (trainingAlgo == BACK_PROP) {
            gradientDecent()
        }
        else {
            out.println("This Training Algorithm is not supported")
        }
    }

    dec[] NeuralNet:predict(dec vect[]){
        return null
    }
    
}
